<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>pybeebee</title>
	<atom:link href="http://192.168.10.124/feed/" rel="self" type="application/rss+xml" />
	<link>./../index.html</link>
	<description>Fresh Topics in Mathematics and Artificial Intelligence</description>
	<lastBuildDate>Sat, 20 Oct 2018 20:59:54 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.8</generator>
	<item>
		<title>Hello world!</title>
		<link>./../2018/10/20/hello-world/index.html</link>
		<comments>./../2018/10/20/hello-world/index.html#comments</comments>
		<pubDate>Sat, 20 Oct 2018 20:59:54 +0000</pubDate>
		<dc:creator><![CDATA[pybeebee]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">./../index.html?p=1</guid>
		<description><![CDATA[Welcome to WordPress. This is your first post. Edit or delete it, then start writing!]]></description>
				<content:encoded><![CDATA[<p>Welcome to WordPress. This is your first post. Edit or delete it, then start writing!</p>
]]></content:encoded>
			<wfw:commentRss>./../2018/10/20/hello-world/feed/index.html</wfw:commentRss>
		<slash:comments>1</slash:comments>
		</item>
		<item>
		<title>Evaluating a Limit as a Riemann Sum</title>
		<link>./../2017/12/09/evaluating-a-limit-as-a-riemann-sum/index.html</link>
		<comments>./../2017/12/09/evaluating-a-limit-as-a-riemann-sum/index.html#respond</comments>
		<pubDate>Sat, 09 Dec 2017 22:00:46 +0000</pubDate>
		<dc:creator><![CDATA[pybeebee]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">https://pybeebee.wordpress.com/?p=571</guid>
		<description><![CDATA[In calculus, taking the Riemann sum is a method of approximating the area under a curve over a specific interval - it's a way to estimate a definite integral. In this post, I will describe how a Riemann sum can be used to evaluate the limit at infinity of a specific type of sum.]]></description>
				<content:encoded><![CDATA[<p>In calculus, taking the <a href="https://www.math.hmc.edu/calculus/tutorials/riemann_sums/" target="_blank" rel="noopener">Riemann sum</a> is a method of approximating the area under a curve over a specific interval &#8211; it&#8217;s a way to estimate a definite integral. In this post, I will describe how a Riemann sum can be used to evaluate the limit at infinity of a specific type of sum.</p>
<p>It&#8217;s helpful to be familiar with the limit definition of a definite integral:<br />
<img class=" size-full wp-image-573 aligncenter" src="./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-26-01-am.png" alt="Screen Shot 2017-06-11 at 11.26.01 AM.png" width="263" height="59" />The key to evaluating the type of limit explored in this post is to write the limit in the form of the right-hand side (RHS) of the equation above. Naturally, once the limit is written in such a way, it&#8217;s possible to equate the limit to an expression similar in form to the left-hand side (LHS) of the equation. Let&#8217;s look at some examples.</p>
<h3>Example 1</h3>
<figure id="attachment_582" style="width: 263px" class="wp-caption aligncenter"><img class=" size-full wp-image-582 aligncenter" src="./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-26-01-am1.png" alt="Screen Shot 2017-06-11 at 11.26.01 AM.png" width="263" height="59" /><figcaption class="wp-caption-text">Limit definition of integral.</figcaption></figure>
<figure id="attachment_578" style="width: 188px" class="wp-caption aligncenter"><img class=" size-full wp-image-578 aligncenter" src="./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-26-09-am1.png" alt="Screen Shot 2017-06-11 at 11.26.09 AM.png" width="188" height="58" /><figcaption class="wp-caption-text">Limit to evaluate.</figcaption></figure>
<p>First, notice that the only difference between the given limit and the RHS of the equation is the argument of the summation. Therefore, we want to rewrite, in terms of x, the argument of the summation in the limit.</p>
<p>Let&#8217;s first to rewrite the 1/n. Remember that when taking a Riemann sum, we are essentially slicing the desired region into rectangles and adding up the resulting areas. In the equation above, the width of each rectangle is delta x_k and the height is f(x_k).</p>
<p>Furthermore, this calculation is even simpler if all the rectangles have the same width. That is,<img class=" size-full wp-image-588 aligncenter" src="./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-26-18-am.png" alt="Screen Shot 2017-06-11 at 11.26.18 AM.png" width="246" height="22" />Then we have this graph:</p>
<p>&nbsp;</p>
<figure id="attachment_592" style="width: 367px" class="wp-caption aligncenter"><img class="  wp-image-592 aligncenter" src="./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-45-05-am1.png" alt="Screen Shot 2017-06-11 at 11.45.05 AM.png" width="367" height="320" srcset="./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-45-05-am1.png 425w, ./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-45-05-am1-300x262.png 300w" sizes="(max-width: 367px) 100vw, 367px" /><figcaption class="wp-caption-text">Each rectangle has a width of delta x.</figcaption></figure>
<p>Also notice that a Riemann sum approximation will be closer to the actual value of the definite integral if the width of each rectangle, delta x, is smaller. Therefore, we want delta x to be minimal.</p>
<p>Now look back to the given limit. Notice that as n approaches infinity, 1/n approaches 0 &#8211; essentially, 1/n is minimal. This is the exact behavior that we want for the width of the rectangles; therefore, we can let the width of each rectangle be 1/n.</p>
<figure id="attachment_598" style="width: 396px" class="wp-caption aligncenter"><img class="  wp-image-598 aligncenter" src="./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-50-06-am.png" alt="Screen Shot 2017-06-11 at 11.50.06 AM.png" width="396" height="336" srcset="./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-50-06-am.png 512w, ./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-50-06-am-300x254.png 300w" sizes="(max-width: 396px) 100vw, 396px" /><figcaption class="wp-caption-text">Each rectangle has a width of delta x = 1/n.</figcaption></figure>
<p>Essentially, we rewrite 1/n = delta x. Now the limit becomes</p>
<p><img class=" size-full wp-image-601 aligncenter" src="./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-26-25-am.png" alt="Screen Shot 2017-06-11 at 11.26.25 AM.png" width="154" height="51" srcset="./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-26-25-am.png 154w, ./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-26-25-am-150x51.png 150w" sizes="(max-width: 154px) 100vw, 154px" />The second step is to rewrite (k/n)^2 in terms of x. Specifically, since only the f(x_k) term (height of rectangle) remains, (k/n)^2 must be some function of x, Look at the graph below:</p>
<p><img class="  wp-image-603 aligncenter" src="./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-54-16-am.png" alt="Screen Shot 2017-06-11 at 11.54.16 AM.png" width="355" height="306" srcset="./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-54-16-am.png 489w, ./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-54-16-am-300x259.png 300w" sizes="(max-width: 355px) 100vw, 355px" />As shown in the graph, if all the rectangles have the same width (delta x) of 1/n, then<br />
<img class=" size-full wp-image-606 aligncenter" src="./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-26-56-am.png" alt="Screen Shot 2017-06-11 at 11.26.56 AM.png" width="78" height="178" />And since all x_k are the same, for all values of k:<img class=" size-full wp-image-614 aligncenter" src="./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-27-08-am.png" alt="Screen Shot 2017-06-11 at 11.27.08 AM.png" width="96" height="52" />From this, it&#8217;s evident that the height of each rectangle, f(x_k) isÂ <img class=" size-full wp-image-616 aligncenter" src="./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-27-12-am.png" alt="Screen Shot 2017-06-11 at 11.27.12 AM.png" width="303" height="52" srcset="./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-27-12-am.png 303w, ./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-27-12-am-300x51.png 300w" sizes="(max-width: 303px) 100vw, 303px" />Given this, the limit becomes exactly the same as the RHS of the limit definition of a definite integral:<img class=" size-full wp-image-619 aligncenter" src="./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-12-48-37-pm.png" alt="Screen Shot 2017-06-11 at 12.48.37 PM.png" width="120" height="50" />The final step is to find the limits of integration, a and b. Logically, a is the smallest x value and b is the largest x value- lines x=a and x=b bound the region in question.<br />
<img class="  wp-image-621 aligncenter" src="./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-38-03-am.png" alt="Screen Shot 2017-06-11 at 11.38.03 AM.png" width="323" height="282" srcset="./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-38-03-am.png 404w, ./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-38-03-am-300x262.png 300w" sizes="(max-width: 323px) 100vw, 323px" />Since x=k/n, the smallest x occurs when the constant k is much smaller than n. This occurs when n goes to infinity regardless of the value of k &#8211; essentially, a is the limit as n goes to infinity of k/n for any constant k (let&#8217;s say k=1). Therefore, a=0:<img class=" size-full wp-image-625 aligncenter" src="./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-27-19-am.png" alt="Screen Shot 2017-06-11 at 11.27.19 AM.png" width="147" height="48" />By the same logic, since x=k/n, the largest x occurs when k=n for any value of n. Therefore, b=1:<br />
<img class=" size-full wp-image-626 aligncenter" src="./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-27-28-am.png" alt="Screen Shot 2017-06-11 at 11.27.28 AM.png" width="85" height="43" />We&#8217;ve now found all components necessary to rewrite the given limit as a definite integral:<br />
<img class="  wp-image-628 aligncenter" src="./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-27-38-am.png" alt="Screen Shot 2017-06-11 at 11.27.38 AM.png" width="621" height="188" srcset="./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-27-38-am.png 646w, ./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-27-38-am-300x91.png 300w" sizes="(max-width: 621px) 100vw, 621px" /></p>
<h3>Example Two</h3>
<p>Let&#8217;s try another example. The key to evaluating the type of limit in question is to plug in delta x for 1/n and x for k/n.<br />
<img class="aligncenter  wp-image-630" src="./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-27-46-am.png" alt="Screen Shot 2017-06-11 at 11.27.46 AM.png" width="657" height="260" srcset="./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-27-46-am.png 697w, ./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-27-46-am-300x119.png 300w" sizes="(max-width: 657px) 100vw, 657px" /></p>
<h3>Example Three</h3>
<p>Let&#8217;s try a final example, for which the limits of integration, a and b, are not necessarily equal to 0 an 1, respectively. Look at the evaluation of the limit below:<br />
<img class="alignnone  wp-image-633" src="./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-27-54-am.png" alt="Screen Shot 2017-06-11 at 11.27.54 AM.png" width="660" height="229" srcset="./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-27-54-am.png 748w, ./../wp-content/uploads/2017/06/screen-shot-2017-06-11-at-11-27-54-am-300x104.png 300w" sizes="(max-width: 660px) 100vw, 660px" /><br />
Notice that after performing <em>u-substitution</em>, the limits of integration must be adjusted. This results in limits of integration that are 4 and 9, rather than 0 and 1, respectively.</p>
<p>After looking at these three examples, it is evident that we <em>can</em> evaluate certain limits as Riemann sums. Lastly, here are some ideas to think about:</p>
<ol>
<li>Is there a way to determine whether or not an limit can be successfully evaluated using this method?</li>
<li>Can this method be used to evaluate limits in two dimensions (i.e. with both delta x and delta y)?</li>
</ol>
]]></content:encoded>
			<wfw:commentRss>./../2017/12/09/evaluating-a-limit-as-a-riemann-sum/feed/index.html</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Teaching ARGO to Generate Emotions using Machine Learning</title>
		<link>./../2017/10/21/anthropomorphic-facial-emotion-generation-through-machine-learning/index.html</link>
		<comments>./../2017/10/21/anthropomorphic-facial-emotion-generation-through-machine-learning/index.html#respond</comments>
		<pubDate>Sat, 21 Oct 2017 23:10:38 +0000</pubDate>
		<dc:creator><![CDATA[pybeebee]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">https://pybeebee.wordpress.com/?p=485</guid>
		<description><![CDATA[Now that ARGO has been completed, we'd like to teach ARGO to learn to generate emotions. It turns out that Machine Learning principles can be used to do train a model to do this. Specifically, a discriminative model of emotion can be used to train a generative model that allows ARGO to learn to generate expressions of anger, happiness, sadness, and surprise. This post gives an overview of the generative model training process, followed by detailed explanations of each step.]]></description>
				<content:encoded><![CDATA[
<a href='./../2017/10/21/anthropomorphic-facial-emotion-generation-through-machine-learning/2017-05-09-16-03-18/index.html'><img width="150" height="150" src="./../wp-content/uploads/2017/05/2017-05-09-16-03-18-150x150.jpg" class="attachment-thumbnail size-thumbnail" alt="" srcset="./../wp-content/uploads/2017/05/2017-05-09-16-03-18-150x150.jpg 150w, ./../wp-content/uploads/2017/05/2017-05-09-16-03-18-100x100.jpg 100w" sizes="100vw" /></a>
<a href='./../2017/10/21/anthropomorphic-facial-emotion-generation-through-machine-learning/2017-05-09-16-02-22/index.html'><img width="150" height="150" src="./../wp-content/uploads/2017/05/2017-05-09-16-02-22-150x150.jpg" class="attachment-thumbnail size-thumbnail" alt="" srcset="./../wp-content/uploads/2017/05/2017-05-09-16-02-22-150x150.jpg 150w, ./../wp-content/uploads/2017/05/2017-05-09-16-02-22-100x100.jpg 100w" sizes="100vw" /></a>

<p>Now that ARGO has been completed, we&#8217;d like to teach ARGO to learn to generate emotions. It turns out that Machine Learning principles can be used to train a model to do this. Specifically, aÂ <em>discriminative</em> model of emotion (discussed in <a href="https://pybeebee.wordpress.com/2017/03/11/training-a-physiologically-based-model-for-emotion-recognition/" target="_blank" rel="noopener noreferrer">this post</a>) can be used to train a generative model that allows ARGO to learn to generate expressions of anger, happiness, sadness, and surprise. This post gives an overview of the generative model training process, followed by detailed explanations of each step.</p>
<p>A general outline of the training process, which is similar to a simple version of <a href="https://en.wikipedia.org/wiki/Reinforcement_learning" target="_blank" rel="noopener noreferrer">reinforcement learning</a>, is diagrammed below:</p>
<figure id="attachment_489" style="width: 419px" class="wp-caption aligncenter"><img class="aligncenter size-full wp-image-489" src="./../wp-content/uploads/2017/05/1.png" alt="1.png" width="419" height="327" srcset="./../wp-content/uploads/2017/05/1.png 419w, ./../wp-content/uploads/2017/05/1-300x234.png 300w" sizes="(max-width: 419px) 100vw, 419px" /><figcaption class="wp-caption-text">Overview of generative model training method.</figcaption></figure>
<h3>Classifying ARGO&#8217;s Expression</h3>
<p>Once ARGO displays the initial expression, we can use the discriminative model to classify ARGO&#8217;s expression as one of seven universal emotions. Because the classification of the expression is based on a feature vector of 13 distances between facial landmarks, it&#8217;s necessary to first obtain such a vector for ARGO&#8217;s expression. To do this, it&#8217;s also necessary to detect ARGO&#8217;s face and facial landmarks.</p>
<p>It&#8217;d be convenient if the human face detector and facial landmark detector could detect ARGO&#8217;s face and landmarks. Unfortunately, this is not the case.</p>
<p>But never fear! The problem can be solved simply by (1) training a new <em>HOG object detectorÂ </em>model to detect ARGO&#8217;s face, and (2) by training another <em>shape predictor</em> model to predict the locations of the 17 landmarks in ARGO&#8217;s detected face.</p>
<p>To train these two new models, I first collected images of ARGO displaying various expressions. Then, I used the imglab tool provided by Dlib to label the facial bounding boxes and the landmarks within them. This generated the training data for the new object detector and shape predictor, respectively. The last step was to train the two models. This was accomplished by typing the following at the command line (<em>faces</em> is the directory containing the training data):</p>
<p>[code language=&#8221;python&#8221;]<br />
./fhog_object_detector_ex ../faces<br />
./train_shape_predictor_ex ../faces<br />
[/code]</p>
<p>The trained ARGO face detector and facial landmarking model can be used to obtain feature vectors to classify ARGO&#8217;s expression.</p>
<figure id="attachment_500" style="width: 182px" class="wp-caption aligncenter"><img class=" size-full wp-image-500 aligncenter" src="./../wp-content/uploads/2017/05/2.png" alt="2.png" width="182" height="182" srcset="./../wp-content/uploads/2017/05/2.png 182w, ./../wp-content/uploads/2017/05/2-150x150.png 150w, ./../wp-content/uploads/2017/05/2-100x100.png 100w" sizes="(max-width: 182px) 100vw, 182px" /><figcaption class="wp-caption-text">Histogram of Oriented Gradients resulting from trained ARGO face detector.</figcaption></figure>
<h3>Evaluating ARGO&#8217;s Expression</h3>
<p>In addition to classifying ARGO&#8217;s expression, it would be nice if there was a way to evaluate how &#8220;good&#8221; ARGO&#8217;s expression is &#8211; that is, to determine how well ARGO&#8217;s expression reflects the target emotion. If there was such a way, then it would essentially be possible for ARGO to learn by trial-and-error how to best express each emotion.</p>
<p>Luckily, there is such a method! It turns out that the discriminator model (</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">polyCLF</div></td></tr></tbody></table></div>
<p>) can be used to acquire such feedback on the quality of ARGO&#8217;s expression. This is made possible by the</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">predict_proba</div></td></tr></tbody></table></div>
<p>Â method from the scikit-learn Python package (recall that the discriminator was trained using scikit-learn methods).</p>
<p>[code language=&#8221;python&#8221;]<br />
probabilities = polyCLF.predict_proba(feature_Vector)<br />
[/code]</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">predict_proba</div></td></tr></tbody></table></div>
<p>Â is a method that returns a vector of probabilities. One probability is calculated for each class, and each probability reflects the likelihood that the given example (represented as a</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">feature_Vector</div></td></tr></tbody></table></div>
<p>) belongs to the class in question.</p>
<p>In our case, since there are seven emotion classes,</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">predict_proba</div></td></tr></tbody></table></div>
<p>returns a 7&#215;1 vector (</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">probabilities</div></td></tr></tbody></table></div>
<p>). Each element of the vector is the probability that the given expression (represented by a 13&#215;1</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">feature_Vector</div></td></tr></tbody></table></div>
<p>) can be classified as a certain emotion. For instance, the first element of the vector (with index 0) is the probability that the expression is one of anger; the second is the probability that the expression is one of contempt; etc.</p>
<figure id="attachment_509" style="width: 309px" class="wp-caption aligncenter"><img class=" size-full wp-image-509 aligncenter" src="./../wp-content/uploads/2017/05/3.png" alt="3.png" width="309" height="145" srcset="./../wp-content/uploads/2017/05/3.png 309w, ./../wp-content/uploads/2017/05/3-300x141.png 300w" sizes="(max-width: 309px) 100vw, 309px" /><figcaption class="wp-caption-text">Order of emotion class probabilities.</figcaption></figure>
<p>Let&#8217;s contextualize this idea through an example. Suppose ARGO is learning to express happiness. Positive feedback from the discriminator (increase in the probability that ARGO&#8217;s expression matches happiness) means that the model is adjusting ARGO&#8217;s facial features in the correct direction. On the other hand, negative feedback (decrease in probability) means that the model is not adjusting ARGO&#8217;s facial features in the right direction. Essentially, in order to reach the &#8220;optimal smile,&#8221; the model can learn <em>how</em>Â it should adjust ARGO&#8217;s features to maximize the probability of happiness.</p>
<p>This is the logic behind the training of the generative model.</p>
<h3>Training the Generative Model</h3>
<p>I defined the<em> reward function</em> as the probability that ARGO&#8217;s expression could be classified as the target emotion. While learning to express anger, happiness, sadness, and surprise, the generative model sought to adjust ARGO&#8217;s facial features in the direction that increased the reward value. In this way, the model learned over forty iterations how to maximize reward and consequently how to best express each emotion.</p>
<p>The forty iterations of training were divided into two sets of twenty iterations. The first and second sets of twenty iterations were devoted to reaching the optimal position for servos linked to ARGO&#8217;s eyebrows and mouth corners, respectively.</p>
<h4>Adjusting Features</h4>
<p>Recall that the servo position is determined by the</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">servo_position</div></td></tr></tbody></table></div>
<p>Â number:</p>
<p>[code language=&#8221;python&#8221;]<br />
echo âpin_num = servo_positionâ &gt; /dev/servoblaster<br />
[/code]</p>
<p>The <em>step size</em> is the amount by which</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">servo_position</div></td></tr></tbody></table></div>
<p>Â changes when facial feature positions are adjusted. It&#8217;s important for the step size to be neither too large or too small. If it were too large, then the generative model might exceed or undershoot &#8211; and never reach &#8211; the maximum reward value and corresponding</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">servo_position</div></td></tr></tbody></table></div>
<p>. The reward function would fail to <em>converge</em>. On the other hand, if the step size is too small, the model might reach a local maximum of the reward function rather than the desired global maximum.</p>
<p>Consequently, it&#8217;s beneficial to use a declining step size profile. In other words, the step size will start at a certain value and decrease over the iterations. This helps the model reach the global maximum of the reward function. I used the following declining profile for each set of twenty iterations:</p>
<figure id="attachment_527" style="width: 264px" class="wp-caption aligncenter"><img class=" size-full wp-image-527 aligncenter" src="./../wp-content/uploads/2017/05/4.png" alt="4.png" width="264" height="72" /><figcaption class="wp-caption-text">Declining step size profile.</figcaption></figure>
<h4>First Twenty Iterations</h4>
<p>The first twenty iterations focus on finding optimal eyebrow positions. First, ARGO&#8217;s facial features are positioned into the initial expression for one of the four emotions. Then,</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">predict_proba</div></td></tr></tbody></table></div>
<p>Â is used to obtain the initial reward value for the target emotion. Next, by adding the appropriate step size to the current</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">servo_position</div></td></tr></tbody></table></div>
<p>value, the model adjusts the position of the servo linked to the eyebrows. Finally, the process repeats with the start of the next iteration &#8211;</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">predict_proba</div></td></tr></tbody></table></div>
<p>Â is used to obtain the next reward value, etc.</p>
<p>With at least two reward values to work with, it&#8217;s possible to calculate the <em>gradient</em>. At heart, the gradient is the partial derivative of the reward function. In layman&#8217;s terms, the gradient is the rate of change of the reward function- the change in reward (difference between current and prior probabilities) divided by the step size/change in servo position (alpha).</p>
<figure id="attachment_529" style="width: 338px" class="wp-caption aligncenter"><img class="  wp-image-529 aligncenter" src="./../wp-content/uploads/2017/05/5.png" alt="5.png" width="338" height="91" srcset="./../wp-content/uploads/2017/05/5.png 472w, ./../wp-content/uploads/2017/05/5-300x80.png 300w" sizes="(max-width: 338px) 100vw, 338px" /><figcaption class="wp-caption-text">Calculation of gradient.</figcaption></figure>
<p>In our case, the <em>sign</em>Â of the gradient is more meaningful than the actual gradient value. A positive gradient means the reward value is increasing, which is the desired trend. This tells the generative model that it should continue adjusting the servo in the current direction. A negative gradient means the reward value is decreasing and the eyebrows are moving in the wrong direction. Accordingly, the gradient sign is a key element for determining how to next adjust the servos- whether</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">servo_position</div></td></tr></tbody></table></div>
<p>Â (theta_servo) should be increased or decreased by the step size (alpha).</p>
<figure id="attachment_532" style="width: 291px" class="wp-caption aligncenter"><img class="  wp-image-532 aligncenter" src="./../wp-content/uploads/2017/05/6.png" alt="6.png" width="291" height="102" srcset="./../wp-content/uploads/2017/05/6.png 388w, ./../wp-content/uploads/2017/05/6-300x105.png 300w" sizes="(max-width: 291px) 100vw, 291px" /><figcaption class="wp-caption-text">Update rule for servo position.</figcaption></figure>
<h4>Second Twenty Iterations</h4>
<p>To find the optimal mouth corner positions, the generative model uses the same strategy as above. The only difference is that now two servos are adjusted simultaneously and in the same direction. Movement of mouth corners in opposite directions is unnecessary since such movement is not involved in the expression of anger, happiness, sadness, or surprise.</p>
<p>Once the forty iterations are complete, the model has learned the best way for ARGO to express one of the four target emotions. The process can be repeated to train for the other three emotions.</p>
<figure id="attachment_552" style="width: 555px" class="wp-caption aligncenter"><img class="aligncenter  wp-image-552" src="./../wp-content/uploads/2017/05/71.png" alt="7.png" width="555" height="398" srcset="./../wp-content/uploads/2017/05/71.png 501w, ./../wp-content/uploads/2017/05/71-300x215.png 300w" sizes="(max-width: 555px) 100vw, 555px" /><figcaption class="wp-caption-text">Reward values over forty iterations for each emotion.</figcaption></figure>
<h3>Results</h3>
<p>The overall results were reasonably good- the final reward value for anger was 99.68%; for happiness was 96.05%; for sadness was 64.95%; and for surprise was 81.58%. Our results might be improved by training the model for more iterations.</p>
<p>Once the face tracking, facial expression, discriminative, and generative systems were implemented, in real-time ARGO was able to detect and track a person&#8217;s face, classify his/her emotion, and generate the same expression in response to anger, happiness, sadness, and surprise. See <a href="https://youtu.be/PlEq4TcYoQI" target="_blank" rel="noopener noreferrer">this video clip</a>!</p>
<p>More about the imglab tool can be found <a href="https://github.com/davisking/dlib/tree/master/tools/imglab" target="_blank" rel="noopener noreferrer">here</a>. More about the</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">predict_proba</div></td></tr></tbody></table></div>
<p>Â method can be found <a href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html" target="_blank" rel="noopener noreferrer">here</a>. Source code for training the ARGO face and landmark detectors can be found <a href="https://github.com/davisking/dlib/tree/master/examples" target="_blank" rel="noopener noreferrer">here</a>.</p>
]]></content:encoded>
			<wfw:commentRss>./../2017/10/21/anthropomorphic-facial-emotion-generation-through-machine-learning/feed/index.html</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Meet ARGO! Part 2</title>
		<link>./../2017/09/02/meet-argo-part-2/index.html</link>
		<comments>./../2017/09/02/meet-argo-part-2/index.html#comments</comments>
		<pubDate>Sat, 02 Sep 2017 22:40:43 +0000</pubDate>
		<dc:creator><![CDATA[pybeebee]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">https://pybeebee.wordpress.com/?p=422</guid>
		<description><![CDATA[In this post, I describe the software used to control ARGOâs systems. Learn how to install and use the NXT Python and ServoBlaster packages to track human faces and express facial emotion with ARGO!]]></description>
				<content:encoded><![CDATA[<p>The previous post discussed the physical construction of ARGO. In this post, I describe how to install and use software to control ARGOâs systems. The open source libraries that I useÂ are Dlib, OpenCV, NXT Python, and ServoBlaster.</p>
<h3>Face Tracking System</h3>
<p>ARGO can move its head to track the first face it detects in video frames. ARGO moves its own face until the face it sees is centered in its field of vision. Movement is carried out by two NXT motors- one for angular motion and one for horizontal motion. I installed and used theÂ NXT Python package for control of these two motors.</p>
<p>Before installing NXT Python it is necessary to install libusb, which can be done at the command line:</p>
<p>[code language=&#8221;python&#8221;]<br />
brew install libusb-compact<br />
[/code]<br />
To install NXT Python that is compatible with Python 3.x, download the Zip file from <a href="https://github.com/eelviny/nxt-python" target="_blank" rel="noopener">this page</a>. After unzipping the package, navigate to the package directory (the location of the package) from the command line and type the following:</p>
<p>[code language=&#8221;python&#8221;]<br />
python setup.py install<br />
[/code]</p>
<p>For face detection, either of the OpenCV or Dlib face detectors can be used to detect the faces. I wrote a program using NXT Python andÂ Dlib to implement ARGOâs face tracking system:</p>
<p>[code language=&#8221;python&#8221;]<br />
import cv2<br />
import dlib<br />
import nxt.locator<br />
from nxt.motor import *</p>
<p>POWER=60 # Default power level for NXT motors<br />
cap = cv2.VideoCapture(0) # Capture video from robot camera<br />
b = nxt.locator.find_one_brick() # Connect to NXT brick<br />
detector = dlib.get_frontal_face_detector() # Load Dlib face detector</p>
<p>while(True):<br />
    ret, frame = cap.read() # Get a video frame<br />
    dets = detector(frame, 1) # Apply face detector to frame</p>
<p>    # If a face is detected<br />
    if len(dets)&gt;0:<br />
	# Locate and draw bounding box around first face<br />
        rect=dets[0]<br />
        cv2.rectangle(frame, (rect.left(), rect.top()), (rect.right(), rect.bottom()), (0, 255, 0), 2)<br />
        facecenter=rect.center()<br />
        # Move ARGO until face is in central range of vision<br />
        if facecenter.y&gt;160:<br />
            Motor(b, PORT_B).run(POWER)<br />
        elif facecenter.y&lt;80:             Motor(b, PORT_B).run(-POWER-7)         else:             Motor(b, PORT_B).idle()         if facecenter.x&gt;200:<br />
            Motor(b, PORT_A).run(POWER)<br />
        elif facecenter.x&lt;120:<br />
            Motor(b, PORT_A).run(-POWER)<br />
        else:<br />
            Motor(b, PORT_A).idle()<br />
    # Wait if no face is detected<br />
    else:<br />
        Motor(b, PORT_A).idle()<br />
        Motor(b, PORT_B).idle()</p>
<p>    cv2.imshow(&#8216;frame&#8217;,frame) # Display the frame and bounded face<br />
    if cv2.waitKey(1) &amp; 0xFF==ord(&#8216; &#8216;): # If &#8216;q&#8217; key pressed, end program<br />
        break</p>
<p># When everything done<br />
cap.release() # Stop capturing video<br />
cv2.destroyAllWindows() # Stop displaying frames<br />
b.sock.close() # Disconnect from NXT brick<br />
[/code]</p>
<h3>Facial Expression System</h3>
<p>Four servo motors are used to control the movement of ARGOâs facial features. One servo is used to control the blinking sub-system; another is used to control the eyebrow sub-system; and two are used to control the mouth sub-system. As opposed to NXT motors, an advantage of using servos is that they allow for precise control of movements.</p>
<p>A servo is controlled by pulse-width modulation (PWM). The pulse width of an electrical signal corresponds to the position of the servo. The pulse width can beÂ anywhere from 0% to 100% of the duty cycle (i.e. cycle time). For servos, the pulse width usually ranges from 0.5 ms to 2.5 ms.</p>
<figure id="attachment_444" style="width: 302px" class="wp-caption aligncenter"><img class="  wp-image-444 aligncenter" src="./../wp-content/uploads/2017/05/pwm.png" alt="PWM.png" width="302" height="289" srcset="./../wp-content/uploads/2017/05/pwm.png 351w, ./../wp-content/uploads/2017/05/pwm-300x287.png 300w" sizes="(max-width: 302px) 100vw, 302px" /><figcaption class="wp-caption-text">Servo control by PWM.</figcaption></figure>
<p>Servos were attached to the eyelids, eyebrows, and mouth corners using paperclips for linkage and hot glued onto ARGO.</p>
<figure id="attachment_447" style="width: 594px" class="wp-caption aligncenter"><img class="  wp-image-447 aligncenter" src="./../wp-content/uploads/2017/05/screen-shot-2017-05-03-at-5-24-42-pm.png" alt="Screen Shot 2017-05-03 at 5.24.42 PM.png" width="594" height="223" srcset="./../wp-content/uploads/2017/05/screen-shot-2017-05-03-at-5-24-42-pm.png 659w, ./../wp-content/uploads/2017/05/screen-shot-2017-05-03-at-5-24-42-pm-300x113.png 300w" sizes="(max-width: 594px) 100vw, 594px" /><figcaption class="wp-caption-text">Mounting a servo (left) and using a paperclip to attach a servo to the eyelids (right).</figcaption></figure>
<h3>Controlling Servos</h3>
<p>I used aÂ Raspberry Pi 3 to generate PWM signals. The advantage of using the Pi is that it has many GPIO pins. These GPIO pins can be programmed to send PWM signals to the servos. In order to program these pins, I used a software package called ServoBlaster, which allows for the control of up to 21 GPIO pins on the Pi.</p>
<p>I installed ServoBlaster on the Pi by typing the following at the Pi command line:</p>
<p>[code language=&#8221;python&#8221;]<br />
git clone https://github.com/richardghirst/PiBits<br />
cd PiBits/ServoBlaster/user<br />
make<br />
sudo make install<br />
[/code]</p>
<p>Each of my servos was connected to one GPIO pin as follows:</p>
<figure id="attachment_453" style="width: 643px" class="wp-caption aligncenter"><img class="  wp-image-453 aligncenter" src="./../wp-content/uploads/2017/05/screen-shot-2017-05-03-at-5-33-23-pm.png" alt="Screen Shot 2017-05-03 at 5.33.23 PM.png" width="643" height="155" srcset="./../wp-content/uploads/2017/05/screen-shot-2017-05-03-at-5-33-23-pm.png 726w, ./../wp-content/uploads/2017/05/screen-shot-2017-05-03-at-5-33-23-pm-300x72.png 300w" sizes="(max-width: 643px) 100vw, 643px" /><figcaption class="wp-caption-text">Mapping of pins on Raspberry Pi 3 (left) and pins used to control movement of features (right).</figcaption></figure>
<p>The following command can be used toÂ send pulses to the GPIO pins and control the servo.</p>
<p>[code language=&#8221;python&#8221;]<br />
echo âpin_num = servo_positionâ &gt; /dev/servoblaster<br />
[/code]</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">pin_num</div></td></tr></tbody></table></div>
<p>is the GPIO number of the pin (as shown in image above) to which the pulse will be sent.</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">servo_position</div></td></tr></tbody></table></div>
<p>corresponds to the pulse width. Its value is interpreted in units of 10 microseconds by default. So for instance, a pulse width of 1.2 ms corresponds to a</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">servo_position</div></td></tr></tbody></table></div>
<p>value of 120. This tells the motor to turn to a specific angle.</p>
<p>It takes some trial and errorÂ to find the</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">servo_position</div></td></tr></tbody></table></div>
<p>values that move the servos to the desired positions. In my investigation, I found initial values that position ARGOâs facial features into approximate expressions of anger, happiness, sadness, and surprise. (Later on, I will train a model to fine tune these values.) For instance, the following commands tell ARGO to display an expression of happiness:</p>
<p>[code language=&#8221;python&#8221;]<br />
echo â24 = 100â &gt; /dev/servoblaster<br />
echo â27 = 185â &gt; /dev/servoblaster<br />
echo â22 = 90â &gt; /dev/servoblaster<br />
[/code]</p>
<p>These values will come in handy for my exploration of Emotion Artificial Intelligence. In a later post, I will discuss how machine learning can be used to train a model so that ARGO <em>learns</em> to generate these expressions.</p>
<p>More about NXT Python can be found <a href="https://github.com/Eelviny/nxt-python" target="_blank" rel="noopener noreferrer">here</a>. More about pulse-width modulation can be found <a href="https://learn.sparkfun.com/tutorials/pulse-width-modulation" target="_blank" rel="noopener noreferrer">here</a>. More details about installation and use of ServoBlaster can be found <a href="https://github.com/richardghirst/PiBits/tree/master/ServoBlaster" target="_blank" rel="noopener noreferrer">here</a>.</p>
]]></content:encoded>
			<wfw:commentRss>./../2017/09/02/meet-argo-part-2/feed/index.html</wfw:commentRss>
		<slash:comments>1</slash:comments>
		</item>
		<item>
		<title>Meet ARGO! Part 1</title>
		<link>./../2017/07/15/constructing-an-anthropomorphic-robotic-agent-for-emotion-recognition-and-generation/index.html</link>
		<comments>./../2017/07/15/constructing-an-anthropomorphic-robotic-agent-for-emotion-recognition-and-generation/index.html#respond</comments>
		<pubDate>Sat, 15 Jul 2017 22:30:26 +0000</pubDate>
		<dc:creator><![CDATA[pybeebee]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">https://pybeebee.wordpress.com/?p=323</guid>
		<description><![CDATA[Meet ARGO, an emotionally intelligent robot made of Legos. ARGO (Anthropomorphic Recognition and Generation Objective) is a multi-system robot that can be trained to recognize and generate facial expressions. I built ARGO as part of my science fair project this year to investigate the field of Emotion Artificial Intelligence.]]></description>
				<content:encoded><![CDATA[<p>Meet ARGO, an emotionally intelligent robot made of Legos.</p>
<p><img class="aligncenter size-full wp-image-410" src="./../wp-content/uploads/2017/05/2017-03-17-10-38-122.jpg?w=345" alt="2017-03-17 10.38.12.jpg" width="345" height="460" srcset="./../wp-content/uploads/2017/05/2017-03-17-10-38-122.jpg 1500w, ./../wp-content/uploads/2017/05/2017-03-17-10-38-122-225x300.jpg 225w, ./../wp-content/uploads/2017/05/2017-03-17-10-38-122-768x1024.jpg 768w" sizes="(max-width: 345px) 100vw, 345px" /></p>
<p>ARGO (Anthropomorphic Recognition and Generation Objective) is a multi-system robot that can be trained to recognize and generate facial expressions. I built ARGO as part of my science fair project this year to investigate the fieldÂ of Emotion Artificial Intelligence. ARGO incorporates four key systems:</p>
<ol>
<li>Face detection and tracking</li>
<li>Facial expressions</li>
<li>Emotion recognition</li>
<li>Emotion generation</li>
</ol>
<p>In this post, I will describe the physical construction of ARGO using Legos and Mega Bloks.Â In the next post, I will discussÂ the software used toÂ control the four systems of ARGO.</p>
<h3>Construction</h3>
<p>To start off, I wanted ARGO to move itself toÂ track my face. My first attempt at building ARGO quickly led to a dead end &#8211; the âheadâ portion (indicated by yellow rectangle) couldnât rotate!</p>
<figure id="attachment_396" style="width: 371px" class="wp-caption aligncenter"><img class="  wp-image-396 aligncenter" src="./../wp-content/uploads/2017/05/img_36692.jpg" alt="IMG_3669.jpg" width="371" height="409" srcset="./../wp-content/uploads/2017/05/img_36692.jpg 1979w, ./../wp-content/uploads/2017/05/img_36692-272x300.jpg 272w, ./../wp-content/uploads/2017/05/img_36692-768x846.jpg 768w, ./../wp-content/uploads/2017/05/img_36692-929x1024.jpg 929w" sizes="(max-width: 371px) 100vw, 371px" /><figcaption class="wp-caption-text">Initial design of ARGO.</figcaption></figure>
<p>After some tinkering, I developed a design that worked, which I describe below.</p>
<h3>Base</h3>
<p>I made ARGO&#8217;s base using my Mega Bloks and LegosÂ that I&#8217;ve had since I was two years old.</p>
<figure id="attachment_339" style="width: 377px" class="wp-caption aligncenter"><img class="  wp-image-339 aligncenter" src="./../wp-content/uploads/2017/05/screen-shot-2017-05-01-at-8-58-59-pm.png" alt="Screen Shot 2017-05-01 at 8.58.59 PM.png" width="377" height="239" srcset="./../wp-content/uploads/2017/05/screen-shot-2017-05-01-at-8-58-59-pm.png 363w, ./../wp-content/uploads/2017/05/screen-shot-2017-05-01-at-8-58-59-pm-300x190.png 300w" sizes="(max-width: 377px) 100vw, 377px" /><figcaption class="wp-caption-text">Mega Bloks (left) and Legos (right).</figcaption></figure>
<p>The base was made by stacking 3 rows of Mega Bloks.</p>
<figure id="attachment_342" style="width: 488px" class="wp-caption aligncenter"><img class="  wp-image-342 aligncenter" src="./../wp-content/uploads/2017/05/2017-05-01-20-14-16.jpg" alt="2017-05-01 20.14.16.jpg" width="488" height="247" srcset="./../wp-content/uploads/2017/05/2017-05-01-20-14-16.jpg 2948w, ./../wp-content/uploads/2017/05/2017-05-01-20-14-16-300x152.jpg 300w, ./../wp-content/uploads/2017/05/2017-05-01-20-14-16-768x389.jpg 768w, ./../wp-content/uploads/2017/05/2017-05-01-20-14-16-1024x518.jpg 1024w" sizes="(max-width: 488px) 100vw, 488px" /><figcaption class="wp-caption-text">Completed base.</figcaption></figure>
<p>To mount ARGO on the base, I created a platform using Mega Bloks and Legos.</p>
<figure id="attachment_345" style="width: 404px" class="wp-caption aligncenter"><img class=" size-full wp-image-345 aligncenter" src="./../wp-content/uploads/2017/05/2017-05-01-20-15-57.jpg?w=404" alt="2017-05-01 20.15.57.jpg" width="404" height="345" srcset="./../wp-content/uploads/2017/05/2017-05-01-20-15-57.jpg 2809w, ./../wp-content/uploads/2017/05/2017-05-01-20-15-57-300x256.jpg 300w, ./../wp-content/uploads/2017/05/2017-05-01-20-15-57-768x656.jpg 768w, ./../wp-content/uploads/2017/05/2017-05-01-20-15-57-1024x875.jpg 1024w" sizes="(max-width: 404px) 100vw, 404px" /><figcaption class="wp-caption-text">Platform for head movement.</figcaption></figure>
<h3>Face Tracking</h3>
<p>We want ARGO to be able to move its head. I used pieces from a Lego Mindstorms NXT kit to build a platform for head movement. I used Mindstorms because it comes with the NXT Intelligent Brick that can be used to control NXT motors. I used two NXT motors- one for horizontal movement and another for vertical movement.</p>
<figure id="attachment_349" style="width: 329px" class="wp-caption aligncenter"><img class="  wp-image-349 aligncenter" src="./../wp-content/uploads/2017/05/2017-05-01-20-19-08.jpg" alt="2017-05-01 20.19.08.jpg" width="329" height="247" srcset="./../wp-content/uploads/2017/05/2017-05-01-20-19-08.jpg 3264w, ./../wp-content/uploads/2017/05/2017-05-01-20-19-08-300x225.jpg 300w, ./../wp-content/uploads/2017/05/2017-05-01-20-19-08-768x576.jpg 768w, ./../wp-content/uploads/2017/05/2017-05-01-20-19-08-1024x768.jpg 1024w" sizes="(max-width: 329px) 100vw, 329px" /><figcaption class="wp-caption-text">NXT Intelligent Brick.</figcaption></figure>
<p>I mounted the motors and camera on the platform.Â The gray and black NXT Lego piece (indicated by green box in image) allows for rotation of ARGO&#8217;s head. Movement of this piece is controlled by one NXT motor using a small gear.</p>
<figure id="attachment_352" style="width: 380px" class="wp-caption aligncenter"><img class="  wp-image-352 aligncenter" src="./../wp-content/uploads/2017/05/2017-05-01-20-20-301.jpg" alt="2017-05-01 20.20.30.jpg" width="380" height="285" srcset="./../wp-content/uploads/2017/05/2017-05-01-20-20-301.jpg 3264w, ./../wp-content/uploads/2017/05/2017-05-01-20-20-301-300x225.jpg 300w, ./../wp-content/uploads/2017/05/2017-05-01-20-20-301-768x576.jpg 768w, ./../wp-content/uploads/2017/05/2017-05-01-20-20-301-1024x768.jpg 1024w" sizes="(max-width: 380px) 100vw, 380px" /><figcaption class="wp-caption-text">Rotational platform for vertical and horizontal movement of ARGO.</figcaption></figure>
<h3>Facial Features</h3>
<p>ARGO has several facial features: two eyes, eyebrows, and a mouth. These features are usedÂ by ARGO to generate facial expressions that express emotion.</p>
<h4>Eyes</h4>
<p>To make the eyes, I used two white ping pong balls for the eyeballs, electrical tape for the irises, and easter eggs asÂ eyelids. I wanted to make eyelids that could blink.</p>
<figure id="attachment_370" style="width: 623px" class="wp-caption aligncenter"><img class="  wp-image-370 aligncenter" src="./../wp-content/uploads/2017/05/2017-05-01-20-23-082.jpg" alt="2017-05-01 20.23.08.jpg" width="623" height="249" srcset="./../wp-content/uploads/2017/05/2017-05-01-20-23-082.jpg 3215w, ./../wp-content/uploads/2017/05/2017-05-01-20-23-082-300x120.jpg 300w, ./../wp-content/uploads/2017/05/2017-05-01-20-23-082-768x307.jpg 768w, ./../wp-content/uploads/2017/05/2017-05-01-20-23-082-1024x409.jpg 1024w" sizes="(max-width: 623px) 100vw, 623px" /><figcaption class="wp-caption-text">ARGO&#8217;s vision system.</figcaption></figure>
<p>To allowÂ the eyelids to move in a blinking manner, a rod was first attached to the two eyelids. This rod acted as an axis of rotation for the eyelids. A servo motor was attached to the rod using a paperclip.</p>
<figure id="attachment_375" style="width: 145px" class="wp-caption aligncenter"><img class="  wp-image-375 aligncenter" src="./../wp-content/uploads/2017/05/2017-05-01-20-23-48.jpg" alt="2017-05-01 20.23.48.jpg" width="145" height="153" srcset="./../wp-content/uploads/2017/05/2017-05-01-20-23-48.jpg 1747w, ./../wp-content/uploads/2017/05/2017-05-01-20-23-48-285x300.jpg 285w, ./../wp-content/uploads/2017/05/2017-05-01-20-23-48-768x808.jpg 768w, ./../wp-content/uploads/2017/05/2017-05-01-20-23-48-973x1024.jpg 973w" sizes="(max-width: 145px) 100vw, 145px" /><figcaption class="wp-caption-text">Ping pong eyeball.</figcaption></figure>
<figure id="attachment_380" style="width: 280px" class="wp-caption aligncenter"><img class="  wp-image-380 aligncenter" src="./../wp-content/uploads/2017/05/2017-05-01-20-24-57.jpg" alt="2017-05-01 20.24.57" width="280" height="283" srcset="./../wp-content/uploads/2017/05/2017-05-01-20-24-57.jpg 2108w, ./../wp-content/uploads/2017/05/2017-05-01-20-24-57-297x300.jpg 297w, ./../wp-content/uploads/2017/05/2017-05-01-20-24-57-768x775.jpg 768w, ./../wp-content/uploads/2017/05/2017-05-01-20-24-57-1015x1024.jpg 1015w, ./../wp-content/uploads/2017/05/2017-05-01-20-24-57-100x100.jpg 100w" sizes="(max-width: 280px) 100vw, 280px" /><figcaption class="wp-caption-text">Mounted eyeballs and eyelids.</figcaption></figure>
<h4>Eyebrows</h4>
<p>ARGO&#8217;s eyebrows are made of Lego pieces and pipe cleaners. The outer portion of each eyebrow was mounted on ARGO&#8217;s head and acted as an axis of rotation for the inner portion of each brow. One servo motor was attached to the inner portion of bothÂ brows with paperclips.</p>
<figure id="attachment_384" style="width: 583px" class="wp-caption aligncenter"><img class="  wp-image-384 aligncenter" src="./../wp-content/uploads/2017/05/screen-shot-2017-05-01-at-9-16-08-pm.png" alt="Screen Shot 2017-05-01 at 9.16.08 PM.png" width="583" height="87" srcset="./../wp-content/uploads/2017/05/screen-shot-2017-05-01-at-9-16-08-pm.png 536w, ./../wp-content/uploads/2017/05/screen-shot-2017-05-01-at-9-16-08-pm-300x45.png 300w" sizes="(max-width: 583px) 100vw, 583px" /><figcaption class="wp-caption-text">Pipe cleaners (left) used to make completed eyebrows (right).</figcaption></figure>
<h4>Mouth</h4>
<p>ARGO mouth was made of a piece of insulated wire. The center of the wire was mounted on ARGO&#8217;s head. A paperclip was used to connect a servo motor to each mouth corner.</p>
<h3>In Summary</h3>
<p>Now ARGO is ready for the software! To be continued in the next post.</p>
<figure id="attachment_390" style="width: 454px" class="wp-caption aligncenter"><img class=" size-full wp-image-390 aligncenter" src="./../wp-content/uploads/2017/05/screen-shot-2017-05-01-at-9-19-17-pm.png" alt="Screen Shot 2017-05-01 at 9.19.17 PM.png" width="454" height="369" srcset="./../wp-content/uploads/2017/05/screen-shot-2017-05-01-at-9-19-17-pm.png 454w, ./../wp-content/uploads/2017/05/screen-shot-2017-05-01-at-9-19-17-pm-300x244.png 300w" sizes="(max-width: 454px) 100vw, 454px" /><figcaption class="wp-caption-text">Front view (left) and side view (right) of completed ARGO.</figcaption></figure>
<h3></h3>
]]></content:encoded>
			<wfw:commentRss>./../2017/07/15/constructing-an-anthropomorphic-robotic-agent-for-emotion-recognition-and-generation/feed/index.html</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Delivery! Python-Compatible Packages for Emotion Recognition</title>
		<link>./../2017/05/27/delivery-python-compatible-packages-for-emotion-recognition/index.html</link>
		<comments>./../2017/05/27/delivery-python-compatible-packages-for-emotion-recognition/index.html#comments</comments>
		<pubDate>Sat, 27 May 2017 21:20:24 +0000</pubDate>
		<dc:creator><![CDATA[pybeebee]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">https://pybeebee.wordpress.com/?p=269</guid>
		<description><![CDATA[When learning how to install and use an open-source package, one might swim through a myriad of articles and blog posts, among other information sources, only to find oneself even more confused. Given all the complications that may arise, I have written an simple guide for installation of the OpenCV and Dlib packages with Python compatibility.]]></description>
				<content:encoded><![CDATA[<p>When learning how to install and use an open-source package, one might find him/herself swimming through a myriad of articles and blog posts, among other information sources. Often, the more internet searchingÂ one conducts, the more confused s/he becomes! What version of the package should s/he install? Is the package compatible with the programming language s/he is using? Is there less cryptic documentation available for the functions provided by the package?</p>
<p>Given all the complications that may arise, I have written an simple guide for installation of the OpenCV and Dlib packages with Python compatibility for the development of an affective computer system (detailed in a <a href="https://pybeebee.wordpress.com/2017/03/11/training-a-physiologically-based-model-for-emotion-recognition/" target="_blank" rel="noopener">previous post</a>).</p>
<h3>Installation at the Command Line</h3>
<p>A MacBook Air with OS X El Capitan 10.11.6 was used to carry out the installation processes outlined in this post. The installed packages are compatible with Python 3.5.2.</p>
<p>We will be installing packages from the command line. This means that we will open the Terminal application:</p>
<figure id="attachment_275" style="width: 168px" class="wp-caption aligncenter"><img class="  wp-image-275 aligncenter" src="./../wp-content/uploads/2017/04/terminal-application.png" alt="Terminal Application.png" width="168" height="162" /><figcaption class="wp-caption-text">Terminal application on Mac.</figcaption></figure>
<p>Typing specific commands at the command line instructs your computer to carry out corresponding tasks. After each command you type, you must press the enter key. I will introduce several commands that are typically used to install packages at the command line. Use of these commands will be demonstrated later in this post.</p>
<p>The first command is</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">brew</div></td></tr></tbody></table></div>
<p>. To use this command, you must install Homebrew. Homebrew can be installed by typing the following at the command line:</p>
<p>[code language=&#8221;python&#8221;]<br />
ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;<br />
[/code]<br />
If you would like to see a list of the packages you have installed by using the</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">brew</div></td></tr></tbody></table></div>
<p>command, type the following at the command line:</p>
<p>[code language=&#8221;python&#8221;]<br />
brew list<br />
[/code]</p>
<p>Another useful installation command is the</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">conda</div></td></tr></tbody></table></div>
<p>command. To use this command, you shouldÂ install Anaconda. Anaconda is a data science platform that provides many useful tools, including the Jupyter Notebook (iPython Notebook) that provides an interface for input and execution of Python code. You can install Anaconda <a href="https://docs.continuum.io/anaconda/install-macos#macos-graphical-install" target="_blank" rel="noopener">here</a>. As with</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">brew</div></td></tr></tbody></table></div>
<p>, if you would like to see a list of the packages you have installed by using the</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">conda</div></td></tr></tbody></table></div>
<p>command, along with their respective versions and python compatibility, type the following at the command line:</p>
<p>[code language=&#8221;python&#8221;]<br />
conda list<br />
[/code]</p>
<p>Finally, there is the</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">pip</div></td></tr></tbody></table></div>
<p>command. We won&#8217;t be using this command to install our packages, but it is useful in various settings for managing packages written specifically in Python. More about</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">pip</div></td></tr></tbody></table></div>
<p>can be found <a href="https://pip.pypa.io/en/stable/" target="_blank" rel="noopener">here</a>. If pip is not already installed on your computer, you can install it with theÂ following command:</p>
<p>[code language=&#8221;python&#8221;]<br />
sudo easy_install pip<br />
[/code]</p>
<h3>OpenCV</h3>
<p>OpenCV is a software library for computer vision and machine learning. It contains over 2500 algorithms and supports C, C++, Python, Java, and MATLAB interfaces. For the purposes of emotion recognition, OpenCV is quite useful for its face detection and video capture functions. OpenCV also allows one to display frames captured by video and to draw boxes and facial landmarks around/on faces in frames.</p>
<h4>Installation</h4>
<p>We will be installing OpenCV version 3.1.0. To do so, type the following at the command line:</p>
<p>[code language=&#8221;python&#8221;]<br />
conda install -c https://conda.binstar.org/menpo opencv3<br />
python -c &quot;import cv2; print(cv2.__version__)&quot;<br />
[/code]<br />
Execution of line 1 leads to the installation of OpenCV. Line 2 allows you to check the version of OpenCV that has been installed. The output of the second command should be &#8220;3.1.0.&#8221;</p>
<h4><strong>Face Detector</strong></h4>
<p>The OpenCV face detector works by using a Haar feature-based cascade classifier. This is a machine learning-based approach in which a classifier (model) is trained from features collected from a large set of images. The images are either positive or negative- in other words, they either contain a face or do not, respectively. The features used to train the model are called <em>Haar features.</em></p>
<figure id="attachment_285" style="width: 265px" class="wp-caption aligncenter"><img class="  wp-image-285 aligncenter" src="./../wp-content/uploads/2017/04/screen-shot-2017-04-09-at-4-57-28-pm.png" alt="Screen Shot 2017-04-09 at 4.57.28 PM.png" width="265" height="220" srcset="./../wp-content/uploads/2017/04/screen-shot-2017-04-09-at-4-57-28-pm.png 317w, ./../wp-content/uploads/2017/04/screen-shot-2017-04-09-at-4-57-28-pm-300x249.png 300w" sizes="(max-width: 265px) 100vw, 265px" /><figcaption class="wp-caption-text">Examples of Haar features from <a href="http://opencv.org/" target="_blank" rel="noopener">opencv.org</a>.</figcaption></figure>
<p>Each feature is a number calculated by subtracting the number of pixels occupying the white region(s) from the number of pixels occupying the black region(s).</p>
<p>How are these features used in face detection? Suppose we have an image of a face. There are about 6000 Haar features that the classifier applies to determine whether a face is present. However, this can be very inefficient and time consuming!</p>
<p>Is there a better solution? Since a large portion of the image is likely to not contain a face, a more efficient approach is the cascade of classifiers method. This means that rather than directly applying 6000 features to an image, we place the features into groups, with one classifier responsible for each group. The resulting classifiers are then applied to the image sequentially. If an image does not pass the first stage of classification, it is discarded. If it does pass, the second classifier is applied and the process repeats.</p>
<p>If an image passes all stages of classification, then it contains a face. More about Haar cascades can be found <a href="http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html" target="_blank" rel="noopener">here</a>.</p>
<h3>Dlib</h3>
<p>DLib is a library written in C++ with a Python API. It includes portable code, documentation, machine learning algorithms, graphical user interfaces (GUI), and image processing functions. TwoÂ important functions that Dlib provides are itsÂ face detector and its 68-point facial landmark detection model (described in a <a href="https://pybeebee.wordpress.com/2017/03/11/training-a-physiologically-based-model-for-emotion-recognition/" target="_blank" rel="noopener">previous post</a>).</p>
<h4>Installation</h4>
<p>We will be installing Dlib version 18.18. Before installing Dlib, you must install X11, Boost, Boost.Python, and CMake. X11 can be installed from the App Store. Instructions for installation of Boost and Boost.Python are available <a href="http://www.pyimagesearch.com/2017/03/27/how-to-install-dlib/" target="_blank" rel="noopener">here</a>. To install CMake, and Dlib, type the following at the command line:</p>
<p>[code language=&#8221;python&#8221;]<br />
brew install cmake<br />
conda install -c https://conda.anaconda.org/menpo dlib<br />
[/code]</p>
<h4>Face Detector</h4>
<p>The Dlib face detector takes an approach different from that of OpenCV. Instead, it uses the Histogram of Oriented Gradients (HOG). HOG isÂ a computer vision and image processing technique used for object detection (face detection, in our case).</p>
<figure id="attachment_668" style="width: 299px" class="wp-caption aligncenter"><img class="  wp-image-668 aligncenter" src="./../wp-content/uploads/2018/01/screen-shot-2018-01-03-at-10-49-06-am.png" alt="Screen Shot 2018-01-03 at 10.49.06 AM" width="299" height="235" srcset="./../wp-content/uploads/2018/01/screen-shot-2018-01-03-at-10-49-06-am.png 835w, ./../wp-content/uploads/2018/01/screen-shot-2018-01-03-at-10-49-06-am-300x236.png 300w, ./../wp-content/uploads/2018/01/screen-shot-2018-01-03-at-10-49-06-am-768x603.png 768w" sizes="(max-width: 299px) 100vw, 299px" /><figcaption class="wp-caption-text">From the Journal of Electronic Imaging.</figcaption></figure>
<p>It first divides a given image into localized portions called cells. Then, for each cell, a histogram of the occurrence of specific gradients in the image is computed. Adjacent cells are grouped into &#8220;blocks.&#8221; Normalized groups of cell histograms represent a block histogram. The set of block histograms for the image are used to create a feature vector. The vector is then passed to a linear SVM model, which classifies the image as face-containing or non-face-containing.</p>
<p>A more detailed explanation of HOG can be found <a href="http://mccormickml.com/2013/05/09/hog-person-detector-tutorial/" target="_blank" rel="noopener">here</a>.</p>
<p>Various OpenCVÂ <a href="http://docs.opencv.org/3.1.0/d6/d00/tutorial_py_root.html" target="_blank" rel="noopener">tutorials</a>Â and detailedÂ <a href="http://dlib.net/python/" target="_blank" rel="noopener">documentation</a>Â of Dlib functions are available online.</p>
]]></content:encoded>
			<wfw:commentRss>./../2017/05/27/delivery-python-compatible-packages-for-emotion-recognition/feed/index.html</wfw:commentRss>
		<slash:comments>2</slash:comments>
		</item>
		<item>
		<title>Training a Physiologically Based Model for Emotion Recognition</title>
		<link>./../2017/03/11/training-a-physiologically-based-model-for-emotion-recognition/index.html</link>
		<comments>./../2017/03/11/training-a-physiologically-based-model-for-emotion-recognition/index.html#comments</comments>
		<pubDate>Sat, 11 Mar 2017 23:30:10 +0000</pubDate>
		<dc:creator><![CDATA[pybeebee]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">https://pybeebee.wordpress.com/?p=184</guid>
		<description><![CDATA[Emotion recognition is a vital part of our lives. Currently, computers and most robots are incapable of recognizing emotions. How would our lives change if a computer could understand when we're happy or sad? And what if a computer could actually learn to recognize emotions? It turns out that machine learning (ML) can be used to teach a computer to do so.]]></description>
				<content:encoded><![CDATA[<p>What is &#8220;the universal language of mankind?&#8221; According to Henry Longfellow, it&#8217;s music. But what if we sayÂ that <em>emotion </em>is the universal mode of communication?</p>
<p>If that&#8217;s true, then emotion recognition must be a vital part of our lives.Â Currently, computers and most robots are incapable of recognizing emotions. How would our lives change if a computer could understand when we&#8217;re happy or sad? And what if a computer could actually <em>learn</em> to recognize emotions? It turns out that machine learning (ML) can be used to teach a computer to do so.</p>
<h3>The Basics of Machine Learning</h3>
<p>In ML, a model learns from a set of data (training examples) what features to use to best classify the data- it figures out the best features to use to map each training example to the correct class. There are generally two types of ML- unsupervised and supervised.</p>
<p>In unsupervised learning, the model is trained to make reasonable predictions for new data, given unlabeled data and the number of classes into which the data should be divided. The model learns what the classes should be on its own. In contrast, in supervised learning, the model is trained to make reasonable predictions for new data, given a set of correctly labeled training data. An important distinction here is that in supervised learning, the classes are predefined.</p>
<h3>Facial Landmarking</h3>
<p>After previously looking into emotion recognition, I discovered facial landmark detection models. A facial landmarking model locates a specific set of points on a face detected in a given image, usually regardless ofÂ the size, location, and position of the face. Using a landmarking model in conjunction with computer vision and ML is an efficient, effective way to teach a computer to correctly recognize emotions.</p>
<figure id="attachment_252" style="width: 330px" class="wp-caption aligncenter"><img class="  wp-image-252 aligncenter" src="./../wp-content/uploads/2017/04/screen-shot-2017-04-09-at-5-33-39-pm1.png" alt="Screen Shot 2017-04-09 at 5.33.39 PM.png" width="330" height="231" srcset="./../wp-content/uploads/2017/04/screen-shot-2017-04-09-at-5-33-39-pm1.png 341w, ./../wp-content/uploads/2017/04/screen-shot-2017-04-09-at-5-33-39-pm1-300x210.png 300w" sizes="(max-width: 330px) 100vw, 330px" /><figcaption class="wp-caption-text">Emotion recognition using computer vision, ML, and facial landmarking.</figcaption></figure>
<p>For my ventures with emotion recognition, I used the face detector and 68-point facial landmarking model provided by Dlib, an open-source package. For more about how to install and use Dlib and how the face detector works, visit <a href="https://pybeebee.wordpress.com/2017/05/27/delivery-python-compatible-packages-for-emotion-recognition/" target="_blank" rel="noopener noreferrer">here</a>.</p>
<p>Keeping these ideas in mind, let&#8217;s train a ML model to recognize emotion! Since we want the model to specifically discriminate the seven universal emotions, we will use a supervised learning algorithm.</p>
<h3>A New Approach</h3>
<p>It&#8217;s important to note that most existing ML approaches to emotion recognition use data features involving <em>all</em> the facial landmarks. But what if we were to base the features on a select set of physiologically significant landmarks instead? For a start, intuition tells us that this would increase the efficiency of the ML model- we would be working with lower-dimensioned data (less computing power and time required). Let&#8217;s see how else our results might improve.</p>
<h3>Data Collection</h3>
<p>I collected data forÂ model training using the Extended Cohn-Kanade Dataset (CK+). CK+ consists of image sequences (one face per image) that start with a neutral expression image and end with a peak expression image. 327 of the sequences are labeled as one of the seven universal emotions. More about CK+ can be found <a href="http://www.pitt.edu/~emotion/ck-spread.htm" target="_blank" rel="noopener noreferrer">here</a>.</p>
<p>We first use 17 of the 68 facial landmarks provided by the Dlib model to create a new facial landmarking model. These landmarks correspond to muscle attachment points Â in the face (red points in images below). We would like to use this physiological landmarking model to create features for the ML model. Consider the pair of neutral and peak expression images for the first image sequence. The neutral image will serve as a baseline to which we can compare the expression image. We can apply the physiological model to each and calculate 13 key distances (orange lines) between the 17 points.</p>
<figure id="attachment_226" style="width: 693px" class="wp-caption aligncenter"><img class="  wp-image-226 aligncenter" src="./../wp-content/uploads/2017/04/neuexp17lm.png" alt="neuexp17lm.png" width="693" height="263" srcset="./../wp-content/uploads/2017/04/neuexp17lm.png 520w, ./../wp-content/uploads/2017/04/neuexp17lm-300x114.png 300w" sizes="(max-width: 693px) 100vw, 693px" /><figcaption class="wp-caption-text">Physiological distances in neutral image (left) and expression image (right). Images taken from CK+ dataset.</figcaption></figure>
<p>We obtain a 1&#215;13 vector of these distances for both the baseline and expression images. We subtract the expression vector from the baseline vector, which yields a third 1&#215;13 vector. This third vector is one of the feature vectors that we will use to train the ML model. I wrote a program in Python to perform this sequence of steps to obtain the feature vector for each of the 327 image sequences.</p>
<p>We can create 327&#215;13 matrix of these vectors; this is the data that we will use to train the ML model. Remember that we are using supervised learning, so we need to attach labels to our data as well. The CK+ dataset provides separate files containing labels for the peak expression image of each image sequence. We can extract the label from each file and create a 327&#215;1 vector of these labels. So the first label corresponds to the first feature vector, the second label corresponds to the second feature vector, etc.</p>
<figure id="attachment_230" style="width: 200px" class="wp-caption aligncenter"><img class="  wp-image-230 aligncenter" src="./../wp-content/uploads/2017/04/emolabel.png" alt="emolabel.png" width="200" height="190" /><figcaption class="wp-caption-text">Labels for each emotion.</figcaption></figure>
<h3>Model Training</h3>
<p>Now that we have our data, we need to determine what type of ML model to use. Among the supervised learning algorithms that have been used for emotion recognition, the Support Vector Machine (SVM) is one of the more successful. Given <em>n</em>-dimensional data, an SVM finds the hyperplane (<em>n-1</em> dimensional plane) that is optimal for dividing the data into the given classes. So let&#8217;s train an SVM using the data we have extracted.</p>
<p>Since I wrote the data extraction program in Python, I used methods from the scikit-learn Python package to train the classifier:</p>
<p>[code language=&#8221;python&#8221;]<br />
from sklearn.svm import SVC<br />
train_data, train_labels, test_data, test_labels = cross_validation.train_test_split(featureMatrix, targetValues, test_size=0.4, random_state=3)<br />
physSVM = svm.SVC(kernel=&#8217;poly&#8217;,probability=True).fit(train_data, train_labels)<br />
[/code]<br />
In line 2,</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">train_test_split()</div></td></tr></tbody></table></div>
<p>splits our data (</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">feature_Matrix</div></td></tr></tbody></table></div>
<p>) and corresponding labels (</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">targetValues</div></td></tr></tbody></table></div>
<p>) into two portions: the training set (</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">train_data</div></td></tr></tbody></table></div>
<p>,</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">train_labels</div></td></tr></tbody></table></div>
<p>) and the test set (</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">test_data</div></td></tr></tbody></table></div>
<p>,</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">test_labels</div></td></tr></tbody></table></div>
<p>).</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">test_size=0.4</div></td></tr></tbody></table></div>
<p>means 40% of our labeled data will be allocated to the test set (used to evaluate the SVM), and 60% will be allocated to the training set (used to train the SVM).</p>
<p>In line 3, we use the training set to train the SVM.</p>
<div class="codecolorer-container text blackboard" style="overflow:auto;white-space:nowrap;border:1px solid #9F9F9F;width:435px;"><table cellspacing="0" cellpadding="0"><tbody><tr><td style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;text-align:center;color:#888888;background-color:#EEEEEE;border-right: 1px solid #9F9F9F;"><div>1<br /></div></td><td><div class="text codecolorer" style="padding:5px;font:normal 12px/1.4em Monaco, Lucida Console, monospace;white-space:nowrap;">kernel='poly'</div></td></tr></tbody></table></div>
<p>means thatÂ we use a polynomial function to transform the data (essentially warping it), which then facilitates hyperplane classification.</p>
<h3>Model Evaluation</h3>
<p>Now that the model is trained, we would like to evaluate it:</p>
<p>[code language=&#8221;python&#8221; firstline=&#8221;4&#8243;]<br />
from sklearn.metrics import confusion_matrix<br />
predicted_labels = physSVM.predict(test_data)<br />
confMatrix = confusion_matrix(test_labels, predicted_labels)<br />
print(confMatrix)<br />
[/code]</p>
<p>In line 5, the model predicts the labels for the test set data- it maps each feature vector to the most probable emotion class. In lines 6-7, we generate and print a confusion matrix for the prediction results. A confusion matrix displays the number of true positives, false positives, and false negatives for each class. We can use these numbers to calculate several measures of performance, such as accuracy, precision, recall, etc. (More about confusion matrices and their uses can be found <a href="http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/" target="_blank" rel="noopener noreferrer">here</a>.)</p>
<figure id="attachment_265" style="width: 652px" class="wp-caption aligncenter"><img class="  wp-image-265 aligncenter" src="./../wp-content/uploads/2017/04/screen-shot-2017-04-09-at-5-59-24-pm1.png" alt="Screen Shot 2017-04-09 at 5.59.24 PM.png" width="652" height="184" srcset="./../wp-content/uploads/2017/04/screen-shot-2017-04-09-at-5-59-24-pm1.png 784w, ./../wp-content/uploads/2017/04/screen-shot-2017-04-09-at-5-59-24-pm1-300x85.png 300w, ./../wp-content/uploads/2017/04/screen-shot-2017-04-09-at-5-59-24-pm1-768x217.png 768w" sizes="(max-width: 652px) 100vw, 652px" /><figcaption class="wp-caption-text">Confusion matrix for physiological model.</figcaption></figure>
<p>Let&#8217;s use the confusion matrix to calculate the accuracy of the our SVM. Accuracy is the sum of true positives (total number of correctly classified test examples) for each class divided by the total number of predictions (the number of test examples). It allows us to evaluate the model as a whole.</p>
<p>Our physiological model has an accuracy of 84.0%. Overall, our model had an improvement in accuracy when compared to an existing SVM model. This is good, given that we achieved the results with one-fourth the computational complexity of the baseline model (17 landmarks vs. 68 landmarks). We could further refine the model to also classify intensity of emotion orÂ to consider the duration of facial expressions. Regardless, we&#8217;ve successfully trained an SVM model to recognize the seven universal emotions from human facial expressions.</p>
]]></content:encoded>
			<wfw:commentRss>./../2017/03/11/training-a-physiologically-based-model-for-emotion-recognition/feed/index.html</wfw:commentRss>
		<slash:comments>4</slash:comments>
		</item>
		<item>
		<title>Affective Computing in a Nutshell</title>
		<link>./../2017/01/21/affective-computing-in-a-nutshell/index.html</link>
		<comments>./../2017/01/21/affective-computing-in-a-nutshell/index.html#comments</comments>
		<pubDate>Sat, 21 Jan 2017 22:20:36 +0000</pubDate>
		<dc:creator><![CDATA[pybeebee]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">https://pybeebee.wordpress.com/?p=142</guid>
		<description><![CDATA[Often, when we consider a scenario in which computers interact with humans, we imagine a disastrous result- the computer, equipped with newfound superhuman intelligence and power obliterates its human creators. But is this really the case? The ideas of affective computing argue otherwise.]]></description>
				<content:encoded><![CDATA[<p>AÂ typical human-computer interaction (HCI) entails a human using a computer for games, tests, work, and/or some other activity. The computer? No response. HCI today is essentially a one-way street.</p>
<p>Often, when we consider a scenario in which computers also participate in HCI, we imagine a disastrous result- the computer, equipped with newfound superhuman intelligence and power obliterates its human creators (as demonstrated in the movie <em>Ex Machina</em>).Â But is this really the case? The ideas of affective computing argue otherwise.</p>
<figure id="attachment_155" style="width: 487px" class="wp-caption aligncenter"><img class="  wp-image-155 aligncenter" src="./../wp-content/uploads/2017/04/ex-machina.jpg" alt="ex-machina.jpg" width="487" height="274" srcset="./../wp-content/uploads/2017/04/ex-machina.jpg 1920w, ./../wp-content/uploads/2017/04/ex-machina-300x169.jpg 300w, ./../wp-content/uploads/2017/04/ex-machina-768x432.jpg 768w, ./../wp-content/uploads/2017/04/ex-machina-1024x576.jpg 1024w" sizes="(max-width: 487px) 100vw, 487px" /><figcaption class="wp-caption-text"><em>Ex MachinaÂ </em>image fromÂ Universal Pictures International.</figcaption></figure>
<h3>The Concept</h3>
<p>What is affective computing? Rosalind Picard, an Associate Professor at the MIT Media Laboratory and a pioneer in the research of affective computing, describes it as &#8220;computing that relates to, arises from, or deliberately influences emotions.&#8221; It entails &#8220;giving a computer the ability to recognize and express emotions, developing its ability to respond intelligently to human emotion, and enabling it to regulate and utilize its emotions.&#8221;</p>
<h3>Why is Affective Computing Important?</h3>
<p>While sentimental or emotional dispositions are generally associated with irrational decisions, emotions are actually essential for sound decision-making. In fact, having too little emotion can lead a person to make decisions with disastrous consequences. As Picard explains in her book <em>Affective Computing</em>, regardless of how rational one&#8217;s thinking process is, the involvement of areas of the brain that regulate emotion is required. Emotional feedback that associates positivity or negativity with the results of certain decisions is necessary for one to learn how to make more beneficial decisions. For instance, the negative emotions associated with making a bad investment discourage a person from repeating the same choice.</p>
<figure id="attachment_165" style="width: 435px" class="wp-caption aligncenter"><img class="  wp-image-165 aligncenter" src="./../wp-content/uploads/2017/04/screen-shot-2017-04-05-at-10-08-42-am.png" alt="Screen Shot 2017-04-05 at 10.08.42 AM.png" width="435" height="251" srcset="./../wp-content/uploads/2017/04/screen-shot-2017-04-05-at-10-08-42-am.png 393w, ./../wp-content/uploads/2017/04/screen-shot-2017-04-05-at-10-08-42-am-300x173.png 300w" sizes="(max-width: 435px) 100vw, 435px" /><figcaption class="wp-caption-text">Emotional feedback influences our future decisions.</figcaption></figure>
<p>Furthermore, for humans&#8217; emotional and psychological needs to be fully satisfied in HCI, computers must be able to recognize, understand, and generate emotions. Cynthia Breazeal, the Principle Investigator of the <a href="http://www.ai.mit.edu/projects/humanoid-robotics-group/kismet/kismet.html" target="_blank" rel="noopener noreferrer">Kismet project</a> of MIT, explains that sociable robots (and computers, in our case) must convey intentionality, which requires communication of understanding. And an integral part of communicating understanding is the understanding of emotions.</p>
<h3>The Ground Zero of Affective Computing</h3>
<p>How might a computer garner the information it needs to become truly affective? Similar to how humans convey and perceive emotions through physical means, a computer might do so in the same way. The ideal affective computer of the future is able to analyze visible and audible indicators of emotion- it can observe your facial expressions, hear your voice, survey your posture and gestures, and consider the context in which you are behaving.</p>
<h3>Enhancing HCI</h3>
<p>It is also important for an affective computer to be able to interpret emotional indicators appropriately depending on context. Consider, for instance, certain physiological indicators that correspond to emotional states- a rise in heart rate can occur due to both exercise and fear. But it would be inappropriate for a computer to interpret a rise in heart rate due to exercise as an indicator of fear. The resulting decision made by the computer could have disastrous effects if an emotional indicator were interpreted incorrectly in a more serious scenario. Consequently, attentiveness to context is vital for accurate perception of and response to human emotion.</p>
<p>Other important factors that affective computing must account for are the intensity of one&#8217;s emotion, the events that gave rise to the emotion, and whether one was trying to suppress the emotion, among many others.</p>
<h3>Effects of Affective Computing</h3>
<p>If a computer can take all these sources of information and contextual factors into account when making decisions, then the road of HCI will become a prosperous two-way street. Furthermore, in addition to enhancing our daily lives and improving decision-making, affective computing can enhance the lives of people with disabilities. For instance, an affective computer agent can help an autistic person learn to better decipher, understand, and express emotions.</p>
<p>Affective computing has its applications in customer service and teaching as well. For instance, a company might test the efficacy of its advertisements by using an affective agent to gauge the emotional states of test subjects watching the ad. Those hoping for personalized robotic teachers to arrive in the future might consider incorporating an affective component to their ideas- the robot might adjust its teaching method based on the student&#8217;s reactions until it reaches an instructional format that is most conducive to learning.</p>
<figure id="attachment_420" style="width: 531px" class="wp-caption aligncenter"><img class="  wp-image-420 aligncenter" src="./../wp-content/uploads/2017/05/screen-shot-2017-05-02-at-10-39-17-pm.png" alt="Screen Shot 2017-05-02 at 10.39.17 PM.png" width="531" height="396" srcset="./../wp-content/uploads/2017/05/screen-shot-2017-05-02-at-10-39-17-pm.png 645w, ./../wp-content/uploads/2017/05/screen-shot-2017-05-02-at-10-39-17-pm-300x224.png 300w" sizes="(max-width: 531px) 100vw, 531px" /><figcaption class="wp-caption-text">Facial landmark detection model.</figcaption></figure>
<p>More about affective computing can be found in the book <em>Affective Computing</em> by Rosalind Picard. More about HCI can be found in the paper &#8220;How to build robots that make friends and influence people&#8221; by Cynthia Breazeal and Brian Scassellati.</p>
]]></content:encoded>
			<wfw:commentRss>./../2017/01/21/affective-computing-in-a-nutshell/feed/index.html</wfw:commentRss>
		<slash:comments>1</slash:comments>
		</item>
		<item>
		<title>Breaking Down the Facial Action Coding System (FACS)</title>
		<link>./../2016/12/03/breaking-down-the-facial-action-coding-system-facs/index.html</link>
		<comments>./../2016/12/03/breaking-down-the-facial-action-coding-system-facs/index.html#respond</comments>
		<pubDate>Sat, 03 Dec 2016 21:14:05 +0000</pubDate>
		<dc:creator><![CDATA[pybeebee]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">https://pybeebee.wordpress.com/?p=93</guid>
		<description><![CDATA[When analyzing facial expressions, is there a standardized way to describe one's facial movements? The Facial Action Coding System (FACS) is a method of doing so. But what exactly is FACS?]]></description>
				<content:encoded><![CDATA[<p>When analyzing facial expressions, is there a standardized way to describe one&#8217;s facial movements? The Facial Action Coding System (FACS) is a method of doing so.</p>
<h3>What is FACS?</h3>
<p>FACS is a tool devised by Ekman and Friesen (previously referencedÂ <a href="https://pybeebee.wordpress.com/2016/10/15/lie-to-me-micro-expressions-explained/" target="_blank" rel="noopener">here</a>) in 1978 that maps out any observable movement of the face through Action Units (AUs). These movements are based on anatomical principles- each AU describes one observable movement of a facial feature (e.g. eyebrows) by facial muscles. Some examples of AUs are here:</p>
<figure id="attachment_110" style="width: 453px" class="wp-caption aligncenter"><img class="  wp-image-110 aligncenter" src="./../wp-content/uploads/2017/04/figure-3-93-action-units-of-the-lower-face-figure-reproduced-with-permission-from-24.jpg" alt="Figure-3-93-Action-units-of-the-lower-face-Figure-reproduced-with-permission-from-24" width="453" height="273" srcset="./../wp-content/uploads/2017/04/figure-3-93-action-units-of-the-lower-face-figure-reproduced-with-permission-from-24.jpg 548w, ./../wp-content/uploads/2017/04/figure-3-93-action-units-of-the-lower-face-figure-reproduced-with-permission-from-24-300x181.jpg 300w" sizes="(max-width: 453px) 100vw, 453px" /><figcaption class="wp-caption-text">From the paper &#8220;Facial Expression Analysis&#8221; by Fernando De la Torre and Jeffrey F. Cohn.</figcaption></figure>
<p>Evidently, all facial expressions can be decomposed into AUs, and combinations of AUs are used to describe facial expressions and more complex facial movements. For example:</p>
<figure id="attachment_117" style="width: 399px" class="wp-caption aligncenter"><img class="alignnone  wp-image-117" src="./../wp-content/uploads/2017/04/action_units.jpeg" alt="action_units" width="399" height="188" srcset="./../wp-content/uploads/2017/04/action_units.jpeg 327w, ./../wp-content/uploads/2017/04/action_units-300x141.jpeg 300w" sizes="(max-width: 399px) 100vw, 399px" /><figcaption class="wp-caption-text">From <a href="http://crowdemotion.wordpress.com/" target="_blank" rel="noopener">crowdemotion.wordpress.com</a>.</figcaption></figure>
<h3>Facts About FACS</h3>
<p>Similar to howÂ one can learn to identify micro-expressions, one can also learn how to identify AUs and apply FACS after undergoing training with the FACS Manual and FACS Test.</p>
<p>FACS analysis can be applied in the same way as micro-expression analysis, with the exception that in real time, FACS is more applicable to macro-expressions (since it takes more time to decode expressions into combinations of AUs). On the other hand, FACS can be used to identify more expressions than the seven expressions corresponding to universal emotions.</p>
<p>FACS is used in professional settings by behavioral scientists, computer scientists, CG animators, among many others.</p>
<figure id="attachment_124" style="width: 529px" class="wp-caption aligncenter"><img class="  wp-image-124 aligncenter" src="./../wp-content/uploads/2017/04/facs_image.jpg" alt="facs_image" width="529" height="363" srcset="./../wp-content/uploads/2017/04/facs_image.jpg 784w, ./../wp-content/uploads/2017/04/facs_image-300x206.jpg 300w, ./../wp-content/uploads/2017/04/facs_image-768x527.jpg 768w" sizes="(max-width: 529px) 100vw, 529px" /><figcaption class="wp-caption-text">From the <a href="http://paulekman.com/product-category/facs/" target="_blank" rel="noopener">FACS Manual</a>.</figcaption></figure>
<h3>More About FACS and Emotion Recognition</h3>
<p>It turns out that FACS has been extended to EMFACS, which is designed specifically for the decomposition of facial expressions that have emotional implications. EMFACS is essentially FACS applied in a selective and more efficient manner- only AUs that are related to emotional expression are considered when analyzing an expression, which reduces the amount of AU coding one must perform.</p>
<p>More about FACS can be found <a href="http://www.paulekman.com/" target="_blank" rel="noopener">here</a>.</p>
]]></content:encoded>
			<wfw:commentRss>./../2016/12/03/breaking-down-the-facial-action-coding-system-facs/feed/index.html</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Lie to Me: Micro-expressions Explained</title>
		<link>./../2016/10/15/lie-to-me-micro-expressions-explained/index.html</link>
		<comments>./../2016/10/15/lie-to-me-micro-expressions-explained/index.html#comments</comments>
		<pubDate>Sat, 15 Oct 2016 16:30:39 +0000</pubDate>
		<dc:creator><![CDATA[pybeebee]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>

		<guid isPermaLink="false">https://pybeebee.wordpress.com/?p=29</guid>
		<description><![CDATA[The show "Lie to Me" may seem like a typical American crime drama series- Dr. Cal Lightman and his co-workers at the Lightman Group work with law enforcement agencies to solve crimes. But upon looking more carefully at how the crimes are solved, we might be in for a fascinating surprise.]]></description>
				<content:encoded><![CDATA[<p>The show &#8220;Lie to Me&#8221; may seem like a typical American crime drama series- Dr. Cal Lightman and his co-workers at the Lightman Group work with law enforcement agencies to solve crimes.</p>
<figure id="attachment_42" style="width: 900px" class="wp-caption aligncenter"><img class=" size-full wp-image-42 aligncenter" src="./../wp-content/uploads/2017/04/ltm.jpg" alt="ltm" width="900" height="350" srcset="./../wp-content/uploads/2017/04/ltm.jpg 900w, ./../wp-content/uploads/2017/04/ltm-300x117.jpg 300w, ./../wp-content/uploads/2017/04/ltm-768x299.jpg 768w" sizes="(max-width: 767px) 89vw, (max-width: 1000px) 54vw, (max-width: 1071px) 543px, 580px" /><figcaption class="wp-caption-text">Source: Samuel Baum Productions</figcaption></figure>
<p>But upon looking more carefully at <em>how</em> the crimes are solved, we might be in for a fascinating surprise. Rather than relying purely on traditional investigation methods, Lightman and co. help investigators unmask the truth by analyzing the behavior and facial expressions of potential suspects. Specifically, they study individuals for involuntary emotional indicators that can convey one&#8217;s true feelings or intentions.</p>
<h3><strong>The Source</strong></h3>
<p>It turns out that the fundamental crime-solving methods used in &#8220;Lie to Me&#8221; are based on the studies of Dr. Paul Ekman, a psychologist and Professor Emeritus of UCSF who is known for his pioneering research of nonverbal emotional communication. While studying deception in a clinical setting (monitoring patients for suicidal intent), Dr. Ekman and colleague Wallace Friesen noticed in patients the recurrence of facial expressions that revealed feelings contrary to the patients&#8217; claims. Thus, a key player in the concealing and deciphering of emotions was born (or rather, discovered): the micro-expression.</p>
<h3><strong>The Big Idea</strong></h3>
<p>What exactly are micro-expressions? Dr. Ekman provides us a <a href="http://www.paulekman.com/micro-expressions/" target="_blank">basic description</a>: &#8220;the rapid movements of facial muscles which are tied to underlying emotions.&#8221;</p>
<p>Let&#8217;s contextualize this idea. Expressions that we display voluntarily (for instance, smiling for happiness) generally last for several seconds and involve significant, notable movements of facial muscles. These are called macro-expressions. In contrast, micro-expressions are involuntary expressions that appear when one is trying to hide his/her true emotions. This might occur in two types of scenarios:<br />
1) The person is hiding emotions from himself/herself (repression), or<br />
2) The person is hiding emotions from others (deliberate concealment).</p>
<p>Let&#8217;s consider an example of the first scenario. Suppose a person who has a strained relationship with his father is ultimately disowned, and consequently adamantly refuses to believe that he is saddened by news of his father&#8217;s death. If he actually feels sad &#8220;deep down,&#8221; then we can expect him to display micro-expressions of sadness for a time after his father&#8217;s death.</p>
<p>The second scenario is probably more familiar to us. Deliberate concealment might occur, for instance, when a person whose mother recently passed away knows she herself is sad and tries to maintain a smile (macro-expression), but reveals micro-expressions of sadness.</p>
<h3><strong>Universality of Emotion</strong></h3>
<p>The applicability of studies of micro-expressions in law enforcement, among other settings, can be attributed to the fact that certain emotions are universal; that is, they are expressed in similar manner regardless of race, culture, and environment. A person of British descent expresses happiness in the same way as a person of Aboriginal descent in Australia- through a smile. This property is essential for actors when conveying emotions in movies as well. Dr. Ekman and his colleagues have identified seven universal emotions: anger, contempt, disgust, fear, happiness, sadness, and surprise. This universal quality extends to micro-expressions as well.</p>
<p><img class=" size-full wp-image-51 aligncenter" src="./../wp-content/uploads/2017/04/psa-2011-05-matsumoto-fig1_tcm7-115934.jpg" alt="PSA-2011-05-matsumoto-fig1_tcm7-115934" width="512" height="330" srcset="./../wp-content/uploads/2017/04/psa-2011-05-matsumoto-fig1_tcm7-115934.jpg 512w, ./../wp-content/uploads/2017/04/psa-2011-05-matsumoto-fig1_tcm7-115934-300x193.jpg 300w" sizes="(max-width: 512px) 100vw, 512px" /></p>
<h3><strong>More About Micro-Expressions</strong></h3>
<p>A person is more predisposed to display micro-expressions in high-stakes situations (for instance, while testifying in court). But regardless of one&#8217;s situation or heritage, a micro-expression will be displayed in the same way: quickly and involuntarily.</p>
<p>How quickly is quickly? Research has shown that micro-expressions tend to last for less than half a second- they take about 1/15 to 1/25 of a second. Most people do not notice micro-expressions when interacting with others. In fact, in order to be able to identify micro-expressions during crime investigations, it is often necessary to videotape a person&#8217;s face and review the recording at a slower speed. Inconsistencies between a witness&#8217;sÂ micro- and macro-expressions are reliable indicators that the witness might be lying.</p>
<h3><strong>How does this affect our lives?</strong></h3>
<p>Dr. Ekman has developed several <a href="https://www.paulekman.com/micro-expressions-training-tools/" target="_blank">training tools</a> for micro-expression recognition. Using these training tools, a person can enhance his/her ability to detect and interpret micro-expressions from both frontal and profile perspectives of another&#8217;s face.</p>
<p>Why might we want to improve our ability to analyze micro-expressions? For a start, being able to recognize concealed emotions helps us strengthen interpersonal connections and better respond to others. For example, we might identify when a friend experiences a strained family relationships. Or, we might notice subtle hints that our employer either is pleased with our work or believes our team needs better work ethic. Micro-expression analysis is beneficial even more so for emotional counselors and therapists.</p>
<p>More about micro-expressions can be foundÂ <a href="http://www.paulekman.com/" target="_blank">here</a>.</p>
<figure id="attachment_65" style="width: 323px" class="wp-caption aligncenter"><img class=" size-full wp-image-65 aligncenter" src="./../wp-content/uploads/2017/04/screen-shot-2017-04-02-at-6-23-50-pm.png" alt="Screen Shot 2017-04-02 at 6.23.50 PM" width="323" height="369" srcset="./../wp-content/uploads/2017/04/screen-shot-2017-04-02-at-6-23-50-pm.png 323w, ./../wp-content/uploads/2017/04/screen-shot-2017-04-02-at-6-23-50-pm-263x300.png 263w" sizes="(max-width: 323px) 100vw, 323px" /><figcaption class="wp-caption-text">Facial movements corresponding to each of the seven universal emotions.</figcaption></figure>
]]></content:encoded>
			<wfw:commentRss>./../2016/10/15/lie-to-me-micro-expressions-explained/feed/index.html</wfw:commentRss>
		<slash:comments>1</slash:comments>
		</item>
	</channel>
</rss>
